---
title: "Final Project: Generalized Additive Models (GAM)" 
author: "**Rachel Holman, Grace Zhang, Rose E.M.**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)    # functions for data manipulation  
library(ggplot2)
library(MASS)
library(dplyr)
library(GGally)
library(kableExtra)
library(DT)
```

# Introduction to Generalize Additive Models (GAM)

In the contemporary business landscape, studying customer shopping trends and preferences is critical for businesses to tailor their products, marketing strategies, and overall customer experience. Our research project endeavors to explore the underlying patterns within the `shopping_trend` dataset, offering a better window into the dynamic world of customer behavior. By delving into anonymized customer shopping trends, we aim to find the factors that influence the amount of purchasing and subscription status by conducting linear and logistic Generalized Additive Models (GAMs) \<....\>

With a collection of 3900 records, this dataset encompasses various features related to customer shopping preferences, gathering essential information for businesses seeking to enhance their understanding of their customer base. The features include customer age, gender, purchase amount, preferred payment methods, frequency of purchases, and feedback ratings. Additionally, data on the type of items purchased, shopping frequency, preferred shopping seasons, and interactions with promotional offers is included.

<https://www.kaggle.com/datasets/iamsouravbanerjee/customer-shopping-trends-dataset>

-   `Customer ID` - Unique identifier for each customer

-   `Age` - Age of the customer

-   `Gender` - Gender of the customer (Male/Female)

-   `Item Purchased` - The item purchased by the customer

-   `Category` - Category of the item purchased

-   `Purchase Amount (USD)` - The amount of the purchase in USD

-   `Location` - Location where the purchase was made

-   `Size` - Size of the purchased item

-   `Color` - Color of the purchased item

-   `Season` - Season during which the purchase was made

-   `Review Rating` - Rating given by the customer for the purchased item

-   `Subscription Status` - Indicates if the customer has a subscription (Yes/No)

-   `Shipping Type` - Type of shipping chosen by the customer

-   `Discount Applied` - Indicates if a discount was applied to the purchase (Yes/No)

-   `Promo Code Used` - Indicates if a promo code was used for the purchase (Yes/No)

-   `Previous Purchases` - The total count of transactions concluded by the customer at the store, excluding the ongoing transaction

-   `Payment Method` - Customer's most preferred payment method

-   `Frequency of Purchases` - Frequency at which the customer makes purchases (e.g., Weekly, Fortnightly, Monthly)

# Specialty of GAM:

GAMs could be used on datasets that consist of predictors that are associated with the outcome variable and the response variable. GAMs are just as versatile as the GLMs because they can be used to model both numeric and categorical response variables. However, GAMs could also be applied to model features that have nonlinear relationship with the response variable. \<interpretable, additive, smoothness...\>

# Advantages and Disadvantages:

The principal advantage of GAM is its ability to model highly complex nonlinear relationships when the number of potential predictors is large. The main disadvantage of GAM is its computational complexity; like other nonparametric methods, GAM has a high propensity for overfitting.

|     | Advantages of GAMs                                                                                                                           | Disadvantages of GAMs                                                                                                                                                                  |
|-------------|-------------------------|----------------------------------|
| 1\. | **Flexibility:** GAMs can model various relationships, including non-linear and complex patterns.                                            | **Complexity:** GAMs can become computationally intensive for large datasets or high-dimensional problems.                                                                             |
| 2\. | **Interpretability:** They provide interpretable results, making understanding the relationships between predictors and the response easier. | **Data Requirements:** GAMs may require larger sample sizes to capture non-linear patterns effectively.                                                                                |
| 3\. | **Non-linearity:** GAMs can capture intricate, non-linear relationships that traditional linear models cannot represent.                     | **Sensitivity to Smoothing Parameters:** The choice of smoothing parameters can impact model results, requiring careful tuning.                                                        |
| 4\. | **Regularization:** GAMs can incorporate regularization techniques to prevent overfitting and improve generalization.                        | **Model Selection:** Selecting the appropriate number and type of smooth terms can be challenging.                                                                                     |
| 5\. | **Visualization:** The smooth functions in GAMs can be visually represented, aiding in model interpretation.                                 | **Limited to Regression and Classification:** GAMs are primarily suited for regression and classification tasks and may not be suitable for more complex tasks like image recognition. |
| 6\. | **Insensitive to Outliers:** GAM outliers do not have as strong of a global influence even if they have a local influence on fit.            | **Wider confidence intervals and less powerful tests.** As a tradeoff for the high flexibility, GAMs generally have wider confidence intervals than simpler models.                    |

More disadvantages:

**Cannot model with non-additive relationships.** GAMs struggle in situations where the relationships between the features and the outcome variables are not additive. They do not capture situations where there are interactions between multiple features where one the value of one feature affects the relationship between another feature and the outcome variable natively.

**Not well known.** Not as commonly used or well known as many other supervised learning models. That means that collaborators may need to set aside some time to learn about the model before they can contribute meaningfully.

**Require some validation.** GAMs are highly flexible and can sometimes converge to solutions that do not make sense. It is generally recommended that you spend some time inspecting the results of your models to make sure that they make sense. Generalized additive models might not be the best option if you do not have bandwidth for this.

**Not guaranteed to converge.** One disadvantage of GAMs is that the models are not guaranteed to converge and you may need to experiment with different model training routines before you arrive on a model that does converge appropriately.

**Sensitive to predictors that are associated with one another.** Much like generalized linear models are sensitive to situations where multiple predictors are linearly correlated with one another, generalized additive models are also sensitive to situations where multiple predictors display similar patterns. Concurvity is a generalization of multicollinearity that is used to describe this specific situation.

**Does not natively handle missing values.** Another disadvantage of GAMs is that they do not natively handle missing values. Most implementations of generalized additive models require the user to select a method that will be used to handle missing data. While there are some more advanced options, the default option is usually to drop rows with missing values.

# R and Python packages for GAMs

*Documentation goes here*

# Data Cleaning and EDA

Before we start performing GAM for linear regression, we loaded our dataset and checked if there are any missing values, abnormal zeros, and NAs in the dataset.

```{r}
# import data
shopping_trend <- read.csv('/Users/ziqia/OneDrive/Desktop/Statistical Learning/project/shopping_trends_updated.csv')

datatable(shopping_trend)
```

```{r}
#data type of variables
str(shopping_trend)
```

```{r}
# check NAs
colSums(is.na(shopping_trend))
```

```{r}
# check zeros
colSums(shopping_trend == 0, na.rm = TRUE)
```

Then, we ran a quick exploratory data analysis by computing the correlation between numeric variables - `Age`, `Purchase.Amount..USD.`, 'Review.Rating', and 'Previous.Purchases'.

```{r}
# correlation between numeric variables
num_corr_tb<- shopping_trend %>% 
  #include dplyr::, otherwise dplyr would clash with MASS package
  dplyr::select(-Customer.ID, -Gender, -Item.Purchased, -Category, -Location,
                -Size, -Color, -Season, -Subscription.Status, -Shipping.Type,
                -Discount.Applied, -Promo.Code.Used, -Payment.Method,
                -Frequency.of.Purchases)


#num_corr_tb 
datatable(cor(num_corr_tb))
```

Next, to make the variables easier to compare and work with, we performed one-hot coding on all categorical variables - `Customer.ID`, `Gender`, `Item.Purchased`, `Category`, `Location`, `Size`, `Color`, `Season`, `Subscription.Status`, `Shipping.Type`, `Discount.Applied`, `Promo.Code.Used`, `Payment.Method`, and `Frequency.of.Purchases`.

```{r}
#one-hot coding 
shopping_trend$Gender <- as.integer(factor(shopping_trend$Gender))-1
#shopping_trend$Item.Purchased <- as.integer(factor(shopping_trend$Item.Purchased)) -1
#shopping_trend$Category <- as.integer(factor(shopping_trend$Category)) -1
#shopping_trend$Location <- as.integer(factor(shopping_trend$Location)) -1
#shopping_trend$Size <- as.integer(factor(shopping_trend$Size)) -1
#shopping_trend$Color <- as.integer(factor(shopping_trend$Color)) -1
#shopping_trend$Season <- as.integer(factor(shopping_trend$Season)) -1
#shopping_trend$Shipping.Type  <- as.integer(factor(shopping_trend$Shipping.Type )) -1
#shopping_trend$Discount.Applied <- as.integer(factor(shopping_trend$Discount.Applied)) -1
#shopping_trend$Promo.Code.Used <- as.integer(factor(shopping_trend$Promo.Code.Used)) -1
#shopping_trend$Payment.Method <- as.integer(factor(shopping_trend$Payment.Method)) -1
#shopping_trend$Frequency.of.Purchases <- as.integer(factor(shopping_trend$Frequency.of.Purchases)) -1
shopping_trend$Subscription.Status <- as.integer(factor(shopping_trend$Subscription.Status)) -1

datatable(shopping_trend)
```




# GAM for Linear Regression (Predicting Purchase Amount)

```{r}
# lets predict purchase.amount with age, gender, review.rating, and previous.purchases
```

### GAM Fit

For GAMs we will make use of the library gam in RStudio, so the first thing that we have to do is to install this package by executing install.packages("gam") once. Then we load the library.

```{r}
library(gam)
#library(mgcv)
```

The main function is gam(). Inside this function we can use any combination of non-linear and linear modelling of the various predictors. For example below we use a smoothing spline for `Age`, `Review.Rating`, and `Previous.Purchases`, and a simple linear model for the variable `Gender`. We then plot the contributions of each predictor using the command plot(). As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor.

```{r}
# fit the GAM model for linear regression
gam_lin = gam( Purchase.Amount..USD. ~ s(Age) + s(Review.Rating) + 
                    s(Previous.Purchases) + Gender,
                  data = shopping_trend )

summary(gam_lin)
```

```{r}
par( mfrow = c(1,4) )
plot( gam_lin,  se = TRUE, col = "blue" )
```

Note that simply using `Gender` inside gam() is just fitting a linear model for this variables. However, one thing that we observe is that the `Gender` variable is binary as it only take the values of 0 and 1. This we can see from the x-axis of the `Gender` plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable to a factor. We first create a second object called shopping_trend1 (in order not to change the initial dataset shopping_trend) and then we use the command factor() to change the variable. Then we fit again the same model. As we can see below now gam() fits a step function for variable `Gender` which is more appropriate.


```{r}
# convert Gender to factor
shopping_trend1 = shopping_trend
shopping_trend1$Gender = factor(shopping_trend1$Gender)

# fit the GAM model for linear regression with updated Gender(factorized)
gam1_lin = gam( Purchase.Amount..USD. ~ s(Age) + s(Review.Rating) + 
              s(Previous.Purchases) + Gender, 
            data = shopping_trend1 )
summary(gam1_lin)
```

```{r}
par( mfrow = c(1,4) )
plot( gam1_lin,  se = TRUE, col = "blue" )
```

We can make predictions from gam objects, just like lm objects, using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 0 to `Gender`, we enclose these values in "" since we informed R to treat  `Gender` as a categorical factor with two levels - "0" and "1".


```{r}
preds_gam_lin <- predict( gam1_lin, 
                  newdata = data.frame( Gender = "0", Previous.Purchases = 27,
                                        Age = 30, Review.Rating = 3.8 )  )
preds_gam_lin
```

### Linear Fit

```{r}
library(gam)
# linear fit
linear_mod = gam( Purchase.Amount..USD. ~ Age + Review.Rating + 
                    Previous.Purchases + Gender,
                  data = shopping_trend1 )

summary(linear_mod)
```

```{r}
preds_lin <- predict( linear_mod, 
                  newdata = data.frame( Gender = "0", Previous.Purchases = 27,
                                        Age = 30, Review.Rating = 3.8 )  )
preds_lin
```


### Model Comparison

Now let's compare our regular regression fit to the GAM fit with smoothing terms. The following shows how one can extract various measures of performance, and the subsequent table shows them gathered together.

```{r}
AIC(linear_mod)
AIC(gam1_lin)
```
 
```{r}
anova(linear_mod, gam1_lin, test="Chisq")
```
With a p-value of 0.1562, the GAM model in this case had not improved the model significantly (p-value > 0.05).


# GAM for Logistic Regression (Predicting Subsription Status)

Likewise, as we noticed earlier - in the GAM model for linear regression - that the `Gender` and `Subsription.Status` variables are binary as it only take the values of 0 and 1. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable to a factor. We first create a second object called shopping_trend2 (in order not to change the initial dataset shopping_trend) and then we use the command factor() to change these two variables. Then we fit GAM model for logistic regression. As we can see below now gam() fits a step function for both variables - `Gender` and `Subsription.Status` `which is more appropriate.

```{r}
# lets predict subscription.status with age, gender, review rating, and previous.purchases
```

```{r}
## convert Gender and Subscription.Status to factor
shopping_trend2 = shopping_trend
shopping_trend2$Gender = factor(shopping_trend2$Gender)
shopping_trend2$Subscription.Status = factor(shopping_trend2$Subscription.Status)

# fit the GAM model for logistic regression
gam_log = gam( Subscription.Status ~ s(Age) + s(Review.Rating) + 
                    s(Previous.Purchases) + Gender,
                  data = shopping_trend2, family = binomial)

summary(gam_log)
```

According to the statistical outcome above, it is found that none of these variables have a significant effect on the subscription status, as all p-values are greater than 0.05.


```{r}
par( mfrow = c(1,4) )
plot( gam_log,  se = TRUE, col = "blue" )
```
Now, we can make predictions from gam objects by using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 1 to `Gender`, we enclose these values in "" since we informed R to treat  `Gender` as a categorical factor with two levels - "0" and "1".


```{r}
# prediction with GAM smoothing terms for logistic regression
preds_gam_log <- predict( gam_log, 
                  newdata = data.frame( Gender = "1", Previous.Purchases = 30,
                                        Age = 20, Review.Rating = 2 )  )
preds_gam_log
```
With assigning the values 1 to `Gender`, 20 to `Previous.Purchases`, 20 to `Age`, and 2 to `Review.Rating`, the predictive subscription status with a GAM model for logistic regression is -0.2673129, which implies to be 0. This indicates that with these assigned value, the subscription status for this customer is likely to be 0, which means the customer does not have a subscription.


### Logistic Fit

```{r}
# logistic fit
log_mod = gam( Subscription.Status ~ Age + Review.Rating + 
                    Previous.Purchases + Gender,
                  data = shopping_trend2, family = binomial )

summary(log_mod)
```

```{r}
# prediction with logistic fit
preds_log <- predict( log_mod, 
                  newdata = data.frame( Gender = "1", Previous.Purchases = 30,
                                        Age = 20, Review.Rating = 2 )  )
preds_log
```

With assigning the values 1 to `Gender`, 20 to `Previous.Purchases`, 20 to `Age`, and 2 to `Review.Rating`, the predictive subscription status with a GAM model for logistic regression is -0.3598807, which implies to be 0. This indicates that with these assigned value, the subscription status for this customer still correctly determine the subscription status to be 0. Hence, the customer does not have a subscription.


### Model Comparison

Now let's compare our regular regression fit to the GAM fit with smoothing terms. The following shows how one can extract various measures of performance, and the subsequent table shows them gathered together.

```{r}
AIC(log_mod)
AIC(gam_log)
```
 
```{r}
anova(log_mod, gam_log, test="Chisq")
```
With a p-value of 0.1499, the GAM model in this case had not improved the model significantly (p-value > 0.05).


# GAM vs. GLM

